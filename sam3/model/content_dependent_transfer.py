import torch
from torch import nn, Tensor
from torch.nn import functional as F
from typing import Optional


from maft.modeling.transformer_decoder.position_encoding import PositionEmbeddingSine





class ShortCut_CrossAttention(nn.Module):

    def __init__(self, d_model, nhead, panoptic_on = True):
        super().__init__()
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=0.0)
        self.norm = nn.LayerNorm(d_model)
        self.activation = F.relu

        self._reset_parameters()

        self.MLP = nn.Linear(d_model, d_model)
        self.panoptic_on = panoptic_on
        if panoptic_on:
            nn.init.constant(self.MLP.weight, 0.0)
            nn.init.constant(self.MLP.bias, 0.0)


    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward(self, tgt, memory,
                memory_mask: Optional[Tensor] = None,
                memory_key_padding_mask: Optional[Tensor] = None,
                pos: Optional[Tensor] = None,
                query_pos: Optional[Tensor] = None):
        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),
                                   key=self.with_pos_embed(memory, pos),
                                   value=memory, attn_mask=memory_mask,
                                   key_padding_mask=memory_key_padding_mask)[0]
        
        tgt2 = self.norm(tgt2) 
        tgt2 = self.activation(tgt2) # <--- 建议加上这一行 (ReLU)
        tgt2 = self.MLP(tgt2)
        tgt = tgt + tgt2

        return tgt
    


class ContentDependentTransfer(nn.Module):

    def __init__(self, d_model, nhead, panoptic_on):
        super().__init__()
        self.pe_layer = PositionEmbeddingSine(d_model//2, normalize=True)
        self.cross_atten = ShortCut_CrossAttention(d_model = d_model, nhead = nhead, panoptic_on = panoptic_on)

    def forward(self, img_feat, text_classifier, ):
        if len(text_classifier.shape) == 2:
            text_classifier = text_classifier.unsqueeze(0).repeat(img_feat.shape[0],1,1)

        pos = self.pe_layer(img_feat, None).flatten(2).permute(2, 0, 1)  # hw * b * c
        img_feat = img_feat.flatten(2).permute(2, 0, 1)  # hw * b * c

        output = self.cross_atten(text_classifier.permute(1, 0, 2), img_feat, memory_mask=None, memory_key_padding_mask=None, pos=pos, query_pos=None)

        output = F.normalize(output, dim=-1) 

        return output.permute(1, 0, 2)
